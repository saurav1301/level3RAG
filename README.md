<<<<<<< HEAD
âœ¨ Features (What makes this Level-3 RAG?)
ðŸ§© 1. Multi-Query Retrieval

Automatically expands the user query into 5+ semantic variants to maximize recall.
This avoids â€œquery missâ€ problems in small datasets.

ðŸ”¥ 2. Cross-Encoder Reranking

Uses Sentence-Transformers similarity scoring to produce a high-precision final ranking.

This fixes:

Wrong document picked due to vector noise

Low-relevance answers

â€œNearest chunk but wrong meaningâ€ RAG failures

ðŸŒ 3. Real-Time Web Search (Tavily API)

The system can fetch live, real-time information from the public internet.

Supports:

Multi-hop reasoning

Multi-source aggregation

Evidence fusion

ðŸ”— 4. Hybrid Fusion Engine

Both local medical PDF knowledge and web evidence are combined into the final answer.

Result = more accurate + up-to-date.

ðŸ›¡ 5. Verification Engine

After generating an answer, the model checks whether each claim is supported by local evidence.

Detects:

Hallucinations

Unsupported statements

Weak citations

ðŸ§  6. Groq Llama-3.1 â€” Ultra Fast LLM

Backed by Groqâ€™s blazing fast AI accelerators.
latency ~5â€“20ms per token.

ðŸ“¦ 7. Pinecone Vector Database (Serverless v4)

Stores chunked embeddings from medical PDFs.

âš™ 8. Flask API â€” Ready for Integration

POST /ask â†’ returns structured JSON:

{
  "answer": "...",
  "local_used": 5,
  "verification": { "ok": true, "count": 4, "matched": [...] }
}

ðŸ§± High-Level Architecture
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  User Question               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          Multi-Query Expansion
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Pinecone Vector DB        â”‚
                    â”‚ (Local Knowledge Retrieval)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ Retrieved Chunks
                              Reranker (Cross Encoder)
                                   â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     Multi-Hop Web Search (Tavily)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                           Evidence Fusion
                                   â”‚
                              Groq Llama 3.1
                                   â”‚
                             Final Answer
                                   â”‚
                           Verification Engine
                                   â”‚
                         JSON Output via Flask

ðŸ“‚ Directory Structure
agentic_RAG/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agent_builder.py        # Old LCEL agent (optional)
â”‚   â”œâ”€â”€ agent_executor.py       # Main Level-3 pipeline
â”‚   â”œâ”€â”€ multi_retriever.py      # Multi-query retriever
â”‚   â”œâ”€â”€ reranker.py             # Cross-encoder reranking
â”‚   â”œâ”€â”€ query_expander.py       # Query paraphrasing
â”‚   â”œâ”€â”€ web_pipeline.py         # Multi-hop web search
â”‚   â”œâ”€â”€ tools.py                # Pinecone + Tavily + helpers
â”‚   â”œâ”€â”€ verifier.py             # Consistency checker
â”‚   â”œâ”€â”€ rag_server.py           # Flask API
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_pinecone_index.py # One-time indexing script
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Medical_book.pdf        # Local knowledge source
â”‚
â””â”€â”€ docker/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ start.sh

ðŸ›  Setup Instructions
1ï¸âƒ£ Install Requirements
pip install -r requirements.txt

2ï¸âƒ£ Set Environment Variables

Create .env:

PINECONE_API_KEY=your_key
PINECONE_INDEX=medical-chatbot
GROQ_API_KEY=your_key
TAVILY_API_KEY=your_key
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

3ï¸âƒ£ Build Pinecone Index (only once)
python scripts/build_pinecone_index.py

4ï¸âƒ£ Run API Server
python -m app.rag_server

ðŸ§ª API Usage (POST Request)
POST /ask

Request:

{
  "question": "What are the symptoms of dengue?"
}


Response:

{
  "answer": "Dengue symptoms include...",
  "local_used": 5,
  "verification": {
    "ok": false,
    "count": 0,
    "matched": []
  }
}

ðŸ§ª Test Cases
âœ” Local-only question

"Explain the causes of anemia."

âœ” Web-only live info

"What are the latest WHO guidelines for dengue in 2025?"

âœ” Local + Web fusion

"Compare local vs latest updates for dengue symptoms."

âœ” Math + Logic Tooling

"A patient takes 250mg medicine 3 times daily. How much per week?"

âœ” Long-context stress test

"Summarize the entire dengue section."

âœ” Verification test

"Does the evidence support: dengue causes purple fingers?"

ðŸ§  Why This Project Is Special (Recruiter Pitch)

This is not a basic chatbot.
It is a full AI retrieval system with:

Dynamic retrieval strategies

Multi-hop search

Evidence-based reasoning

Grounded outputs

Industry architecture (Perplexity-style)

Real-time web sourcing

LLM + vector + reranking synergy

Recruiters see:

LLM Ops

Production AI engineering

Retrieval pipelines

API design

Search engineering

Embedding models

Pinecone expertise

Groq inference

Web search agents

You look like someone who can design & deploy scalable AI systems, not just toy apps.

Thatâ€™s how 20 LPA happens. ðŸ˜‰

ðŸ“œ License

MIT License.

â­ If you like this projectâ€¦

Leave a â­ on the repo and connect on LinkedIn.
=======
# level3RAG
>>>>>>> 6106c705339d1def4dad91c189bfb049839a080e
